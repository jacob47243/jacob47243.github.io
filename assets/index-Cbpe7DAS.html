<!-- src/project-pages/snapar3d/index.html -->
<section id="project-snapar3d" data-base="/src/project-pages/snapar3d/">
  <style>
    /* Match Dynamic RRT’s structure & visuals */
    #project-snapar3d { color:#e8e9ea; }
    #project-snapar3d .grid{ display:grid; gap:16px; grid-template-columns:1fr; }
    #project-snapar3d h2{ margin:6px 0 4px; font-size:22px; }
    #project-snapar3d h3{ margin:10px 0 6px; font-size:16px; color:#a0a3a7; }
    #project-snapar3d p{ margin:0 0 10px; }
    #project-snapar3d ul{ margin:6px 0 12px; padding-left:18px; }
    #project-snapar3d li{ margin:4px 0; }
  
    /* Cards (same as Dynamic RRT) */
    .card{
      background:#121315; border:1px solid rgba(255,255,255,.08);
      border-radius:14px; padding:16px;
    }
  
    /* Media block + frame (no teal border; same shadow/background) */
    .media{ display:grid; gap:12px; }
    .media .frame{
      position:relative;
      border:none;                         /* ← remove any teal outlines */
      border-radius:12px;
      background:#0f1113;
      box-shadow:0 12px 40px rgba(0,0,0,.35);
      overflow:hidden;
      aspect-ratio:16/9;
      min-height:220px;
    }
    .media video,
    .media iframe{
      position:absolute; inset:0; width:100%; height:100%; border:0;
    }
  
    /* Button row + buttons (same behavior as Dynamic RRT) */
    .btn-row{ display:flex; gap:8px; flex-wrap:wrap; margin-top:6px; }
    .btn{
      display:inline-block; padding:8px 12px; border-radius:10px; font-weight:700;
      border:1px solid rgba(255,255,255,.15);
      background: var(--accent, #14b8a6);
      color:#0b0b0c !important;            /* dark text by default */
      text-decoration:none;
    }
    .btn:hover{
      filter: brightness(1.05);
      color:#fff !important;               /* white on hover */
    }
  </style>
  
  
    <!-- Header -->
    <header class="card">
      <h2>SnapAR3D</h2>
      <p>
        SnapAR3D uses AR to guide optimal photo capture and record precise camera poses relative to your object.
        We convert this data for NVIDIA’s nvdiffrast / nvdifrec pipeline to create accurate 3D models.
      </p>
    </header>
  
    <!-- Media -->
    <section class="card media">
      <h3>Demo</h3>
  
      <!-- YouTube embed -->
      <div class="frame">
        <iframe
          src="https://www.youtube.com/embed/9cUk6gcapfg"
          title="SnapAR3D — overview"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; fullscreen"
          allowfullscreen>
        </iframe>
      </div>
  
      <div class="btn-row">
        <a class="btn" href="https://devpost.com/software/snapar3d" target="_blank" rel="noopener">View on Devpost</a>
        <a class="btn" href="https://www.youtube.com/watch?v=9cUk6gcapfg" target="_blank" rel="noopener">Watch on YouTube</a>
      </div>
    </section>
  
    <!-- Narrative blocks -->
    <section class="grid">
      <article class="card">
        <h3>Inspiration</h3>
        <p>
          We were inspired by the growing need for accessible 3D capture and the potential of AR to help users
          take great photos. Guiding viewpoints + capturing poses lets everyday imagery drive high-quality 3D recon.
        </p>
      </article>
  
      <article class="card">
        <h3>What it does</h3>
        <p>
          The app uses AR to guide photo capture and records per-image camera extrinsics. We convert this into
          NeRF-style transforms for nvdifrec, producing accurate meshes/textures from a small number of photos.
        </p>
      </article>
  
      <article class="card">
        <h3>How we built it</h3>
        <ul>
          <li>ARKit/RealityKit to acquire images + device pose.</li>
          <li>COLMAP formatting & a conversion step to NeRF-compatible transforms (JSON).</li>
          <li>Reconstruction via nvdifrec; lightweight host to share <code>transforms.json</code>.</li>
        </ul>
      </article>
  
      <article class="card">
        <h3>Challenges</h3>
        <ul>
          <li>Pose/frame fine-tuning for fidelity vs. speed.</li>
          <li>Coordinate frame differences between ARKit and NeRF conventions.</li>
          <li>Integrating data from multiple sources without drift.</li>
        </ul>
      </article>
  
      <article class="card">
        <h3>Accomplishments</h3>
        <ul>
          <li>AR guidance that helps non-experts capture high-quality inputs.</li>
          <li>Recon quality comparable to heavier pipelines, with faster training time.</li>
        </ul>
      </article>
  
      <article class="card">
        <h3>What we learned</h3>
        <p>
          A deeper understanding of AR pipelines, camera models, and how robust 3D reconstructions emerge from
          careful capture + clean pose data. Also: iterate relentlessly—small data/layout tweaks matter a lot.
        </p>
      </article>
  
      <article class="card">
        <h3>What’s next</h3>
        <ul>
          <li>Smoother on-device capture UI and guidance.</li>
          <li>Better support for diverse objects/scenes and lighting.</li>
          <li>More real-time feedback during capture and conversion.</li>
        </ul>
      </article>
    </section>
  
    <!-- Built With / Skills -->
    <section class="card">
      <h3>Tools</h3>
      <p class="tags">
        <span class="tag">ARKit</span>
        <span class="tag">RealityKit</span>
        <span class="tag">Swift</span>
        <span class="tag">SwiftUI</span>
        <span class="tag">COLMAP</span>
        <span class="tag">OpenCV</span>
        <span class="tag">Python</span>
        <span class="tag">JavaScript</span>
        <span class="tag">nvdiffrast / nvdifrec</span>
        <span class="tag">Transforms.json</span>
        <span class="tag">3D Reconstruction</span>
      </p>
    </section>
  </section>
  